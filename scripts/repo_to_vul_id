#!/usr/bin/env python
"""
file: repo_to_vul_id
author: adh
created_at: 9/7/21 1:00 PM
"""
import argparse
import glob
import logging
import os
import random
import pandas as pd
from pandas.errors import EmptyDataError

import labyrinth
from labyrinth.patterns import normalize, id_to_path


logger = logging.getLogger()
logger.setLevel(logging.INFO)
hdlr = logging.StreamHandler()
fmt = logging.Formatter("%(levelname)s %(name)s - %(message)s")
hdlr.setFormatter(fmt)
logger.addHandler(hdlr)

_IN_DIR = "data/repo_id"


def main(in_dir):
    df = read_repo_summaries(in_dir)
    write_vul_id_summaries(df)


def write_vul_id_summaries(df):
    for gname, group in df.groupby("match"):
        _write_vul_id_summary(gname, group)


def _write_vul_id_summary(vul_id, df):
    outpath = os.path.join("data", "vul_id", id_to_path(vul_id))
    outfile = os.path.join(outpath, f"{vul_id}.csv")
    if labyrinth.DEBUG:
        _maybe_log(df, outfile, vul_id)
    os.makedirs(outpath, exist_ok=True)
    df.to_csv(outfile, index=False, float_format="%0.8f")


def _maybe_log(df, outfile, vul_id):
    if random.random() < 0.001:
        print(vul_id, outfile)
        print(df.to_csv(index=False, float_format="%0.8f"))


def _read_repo_summary(csvfile):
    try:
        df = pd.read_csv(csvfile)
    except EmptyDataError as e:
        return None

    df["match"] = df["match"].apply(normalize)
    df = df.drop_duplicates(subset=["repo_id", "match"])
    return df


def read_repo_summaries(in_dir):
    glob_pattern = f"{in_dir}/**/*.csv"
    repo_files = glob.glob(glob_pattern, recursive=True)
    logger.info(f"Found {len(repo_files)} data files")
    # read them all
    _data = []
    for i, f in enumerate(repo_files):
        if i % 1000 == 0:
            logger.info(f"{i} files read so far from {in_dir}")

        _df = _read_repo_summary(csvfile=f)
        if _df is None:
            continue

        logger.debug(f"Adding {len(_df)} records from {f}")
        _data.append(_df)
    df = pd.concat(_data)
    df = _clean_df(df)
    return df


def _clean_df(df):
    # renormalize match strings
    df["match"] = df["match"].apply(normalize)
    df = df.drop_duplicates(subset=["repo_id", "match"])
    df = df.join(
        df.groupby("repo_id")["match"].nunique(), on="repo_id", rsuffix="_nunique"
    )
    df["match_weight"] = 1.0 / df["match_nunique"]
    df["repo_url"] = df["repo_full_name"].apply(lambda x: f"https://github.com/{x}")
    keep_cols = ["match", "match_weight", "repo_url", "repo_full_name", "repo_id"]
    df = pd.DataFrame(df[keep_cols].reset_index(drop=True))
    df = df.sort_values(by=["match_weight", "repo_id"], ascending=False)
    return df


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Process repo/file/match results to list per vul ID"
    )
    parser.add_argument("--input_dir", action="store", type=str, default=_IN_DIR)

    group = parser.add_mutually_exclusive_group()
    group.add_argument("--verbose", "-v", action="store_true", default=False)
    group.add_argument("--debug", "-d", action="store_true", default=False)

    args = parser.parse_args()

    if args.verbose:
        labyrinth.VERBOSE = True
        logger.setLevel(logging.INFO)
        logger.info("log level: INFO")
    if args.debug:
        labyrinth.DEBUG = True
        logger.setLevel(logging.DEBUG)
        logger.debug("log level: DEBUG")

    main(args.input_dir)
