#!/usr/bin/env python
"""
file: crawl_files
author: adh
created_at: 8/24/21 9:43 AM
"""
import argparse
import logging

import labyrinth
from labyrinth.repo_processor import process_summary

logger = logging.getLogger()
logger.setLevel(logging.INFO)
hdlr = logging.StreamHandler()
fmt = logging.Formatter("%(levelname)s %(name)s - %(message)s")
hdlr.setFormatter(fmt)
logger.addHandler(hdlr)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Process a set of git repos from a summary and look for patterns in all files within it"
    )

    # TODO: make this per day?
    parser.add_argument("summary_file", action="store", type=str)
    parser.add_argument("--token", action="store", type=str, default=None)
    parser.add_argument("--verbose", "-v", action="store_true", default=False)
    parser.add_argument("--debug", "-d", action="store_true", default=False)

    args = parser.parse_args()

    if args.verbose:
        labyrinth.VERBOSE = True
        logger.setLevel(logging.INFO)
    if args.debug:
        labyrinth.DEBUG = True
        logger.setLevel(logging.DEBUG)

    if args.token is not None:
        labyrinth.GH_TOKEN = args.token

    process_summary(args.summary_file)
